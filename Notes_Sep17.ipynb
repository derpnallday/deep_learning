{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent and backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Networks Learn\n",
    "specifically: how to set weights so network makes go predictions\n",
    "\n",
    "**STEPS**\n",
    "1. Gradient descent\n",
    "2. Backpropagation\n",
    "\n",
    "$$ $$\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "**How do we find the weights that give the best classification**\n",
    "\n",
    "Given the predicted value and the true value, run values through a cost function to determine error (such as mean squared error). Divided over number of data points gives average error\n",
    "\n",
    "Given predicted $X$ given $W$\n",
    "\n",
    "$$C(W)=\\frac{1}{N}\\sum_i (\\hat{y}_i-y_i)^2$$\n",
    "\n",
    "Using gradient descent, step in the direction of the negative gradient of the cost function given a set step size until reaching a minimum (can be stuck in the local minimum).\n",
    "\n",
    "To update the weights:\n",
    "\n",
    "$$W = W^{old} + \\lambda (-\\nabla C(W^{old})$$\n",
    "\n",
    "where $\\lambda$ is a predetermined learning rate\n",
    "\n",
    "**How it comes together**\n",
    "\n",
    "1) The model\n",
    "Input (datpoints, $X$) -> Neural network function (parameters: Weights $W$) -> Output (prediction, $\\hat{y}$)\n",
    "\n",
    "2) its performance\n",
    "Input -> cost function -> output\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "**Chain rule** - taking derivatives of composite functions\n",
    "\n",
    "A neural network is essentially a composite function\n",
    "\n",
    "ex:\n",
    "\n",
    "$$f(x) = h(g(x)))$$\n",
    "\n",
    "chain rule states \n",
    "\n",
    "$$\\frac{df}{dx} = \\frac{df}{dh} \\frac{dh}{dg} \\frac{dg}{dx}$$\n",
    "\n",
    "ex 2:\n",
    "$$m(z) = -z $$\n",
    "$$g(z) = exp(z) $$\n",
    "$$h(z) = z+1 $$\n",
    "$$f(z) = \\frac{1}{z}$$\n",
    "\n",
    "input:\n",
    "$$x_0=1$$\n",
    "$$x_1=1.1$$\n",
    "\n",
    "Network:\n",
    "$$x-(1)>m-(-1)>g-(0.37)>h-(1.37)>f-()0.73>$$\n",
    "\n",
    "Model derivatives\n",
    "$$m'(z)=-1$$\n",
    "$$g'(z)=exp(z)$$\n",
    "$$h'(z)=1$$\n",
    "$$f'(z)=-\\frac{1}{z^2}$$\n",
    "\n",
    "Q: how does a small nudge in $x$ influence $f(h(g(m(x))))$?\n",
    "A: propagate gradients backwards using chain rule\n",
    "\n",
    "chain rule from $f$ to $m$\n",
    "$$\\frac{df}{dg}= \\frac{df}{dh}\\frac{dh}{dg}\\frac{dg}{dm}\\frac{dm}{dx}$$\n",
    "\n",
    "$$\\frac{df}{dx} = -\\frac{1}{1.37^2} * 1 * e^{-1} = .20$$\n",
    "\n",
    "This example is actually the sigmoid function\n",
    "\n",
    "**Backpropagation on a neural network**\n",
    "\n",
    "ex model:\n",
    "$$w_0 + x_0 w_1 + x_1 w_2 + x_2 w_3 = z(\\textbf{x})$$\n",
    "$$\\sigma(x) = \\frac{1}{1+exp(-x)}$$\n",
    "$$C(\\hat{y},y)=(\\hat{y} - y )^2$$\n",
    "\n",
    "Derivatives:\n",
    "$$y=\\sum^N_{i=0}z_n    \\text{       (each branch is a function)}$$\n",
    "$$\\sigma'(x)=\\sigma(x)(1-\\sigma(x))$$\n",
    "$$C'(\\hat{y},y)=2(\\hat{y} - y )$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
