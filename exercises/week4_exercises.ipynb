{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **DO NOT EDIT IF INSIDE annadl_f19 folder**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Convolutional Neural Networks\n",
    "\n",
    "We learned about feedforward neural networks over the last two weeks, along with most of the fundamental maths, algorithms, tricks, etc. that go into training them. Knowing how signal propagates forward, how gradients flow backwards and how weights get updates, we are now ready to release our imagination and create some deep neural networks that can do remarkable things. We will first focus on one of the most **powerful architectures**, the Convolutional Neural Network (CNN), which has completely revolutionalized image recognition in science and tech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T20:07:00.424129Z",
     "start_time": "2019-09-30T20:07:00.418737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with feed forward images when classifying images is that the input layer looks at the whole image at once. Each input neuron thus becomes associated with a *specific* pixel, and as the network learns it may grow to expect a certain signal to emerge at that particular pixel. But if you imagine an image of a cat, features, like the whiskers, which make it cat-like, are not bound to a specific *place* in the image â€“ they can be in the centre, top corner, or any other place.\n",
    "\n",
    "CNNs solve this problem, with what's called *convolutional layers*. A convolutional input layer, for example, doesn't have a single weight for each input value. Instead, it has one or more much smaller *filters*, each one a set of input neurons (often $3 \\times 3 \\times d$ where $d$ is the depth of the input image (3 if it's an RGB image, 1 if its black and white)) that gets *convolved* across the input image to produce a new image, called an *activation map*. [Here](https://github.com/vdumoulin/conv_arithmetic) are some nice gifs that illustrate different ways one can convolve a weight matrix across an input image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pen and paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get your intuition for computations on input data in CNNs fine-tuned, I have a few small quizzes for you. First, we'll consider the size of the parameter space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.1.1**: Imagine you have a CNN with just one convolutional layer with a single filter. All it does, is take an input image and produce an activation map. The dimensionality of the filter in your convolutional layer is $5 \\times 5 \\times 3$. How many weights (or *parameters*) are there in this model?\n",
    ">\n",
    "> *Hint*: Don't forget the bias!\n",
    ">\n",
    "> $$5*5*3 + 1 \\text{ (bias for each filter) } = 76$$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the formula for computing the size of the activation map resulting from a convolution. \n",
    "If you have a filter that is $F$ wide, your input image is $W_0$ wide, you are padding the edges by\n",
    "$P$ pixels and your stride is $S$, the resulting image will have width/height:\n",
    "\n",
    "$$ W_1 = \\frac{W_0 - F + 2P}{S} + 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.1.2**: You input an image of dimensions $28 \\times 28 \\times 3$, use a padding of 2, a stride of 1,\n",
    "and then slide your $5 \\times 5 \\times 3$ filter across the image. What is the dimensionality of the resulting activation map?\n",
    ">\n",
    "> $$\\frac{28 - 5 + 2(2)}{1}+1 = 28 \\times 28 \\times 1 $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.1.3**: Let's say you now want to use a stride of 2, instead of 1. What problem does this immediately cause?\n",
    ">\n",
    "> The filter will slide over the image on the last iteration and will not fit all of the pixels in range\n",
    ">\n",
    ">How can we solve it?\n",
    ">\n",
    "> we can solve it with additional padding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Maxpooling* is a method used a lot in CNNs, which downsamples the size of an activation map. It is used primarily to reduce the amount of parameters and computations needed in the network, and to avoid overfitting. Here's an illustration of how it works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](http://cs231n.github.io/assets/cnn/maxpool.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, for each $2 \\times 2$ square in your activation map, you pick the largest value in that square. You do this independently for every depth slice in your activation map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In Keras, the dimension of data is a little different from what you may expect. The first index,\n",
    "indexes datapoints, the second and third are the dimensions of your images, and the last is number of channels. So if\n",
    "you have a batch of data containing 100 datapoints, each one an RGB image (so 3 channels: red, green, blue)\n",
    "with resolution $128 \\times 128$, then the dimensionality of your input data is (100, 128, 128, 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.1.4**: Given the activation map below, what is the corresponding activation map after maxpooling ($2 \\times 2$ filter, stride 2)? Run it through a Keras maxpooling layer (check out the docs), and report the dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T19:37:21.198895Z",
     "start_time": "2019-09-30T19:37:21.107703Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'max_pooling2d_3/MaxPool:0' shape=(10, 14, 14, 1) dtype=float32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.random(size=(10, 28, 28, 1))\n",
    "activation_map = keras.backend.variable(a)\n",
    "\n",
    "pool = keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2, padding='valid', data_format=None)\n",
    "\n",
    "pool(activation_map)\n",
    "\n",
    "#shape=(10, 14, 14, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNNs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example sake, I have implemented a single conv. layer neural network Keras below. The code below assumes input images of shape $28 \\times 28 \\times 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T19:57:40.146609Z",
     "start_time": "2019-09-30T19:57:40.141327Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(filters=10, kernel_size=3, strides=(1, 1), padding='valid'),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "    Flatten(),\n",
    "    Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercise you will use the MNIST dataset again. In the cell below I have written some code to prepare it somewhat. For your own sake, try to understand what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T20:12:46.465418Z",
     "start_time": "2019-09-30T20:12:46.027204Z"
    }
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape data so it has a channel dimension\n",
    "rows, cols = x_train.shape[-2:]\n",
    "x_train = x_train.reshape(x_train.shape[0], rows, cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], rows, cols, 1)\n",
    "\n",
    "# Convert pixel intensities to values between 0 and 1\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "    \n",
    "# Convert target vectors to one-hot encoding\n",
    "num_classes = len(set(y_train))\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.1.5**: Implement Nielsen's [last convolutional neural network](http://neuralnetworksanddeeplearning.com/chap6.html#convolutional_neural_networks_in_practice)\n",
    "(the one with two convolutional layers and dropout), and score an accuracy higher than 98%. It doesn't have to be\n",
    "fully identical, but his solution is pretty great, so getting close is a cheap way to score a high accuracy.\n",
    ">\n",
    "> *Hint:* [here](https://keras.io/examples/mnist_cnn/) is an example of a CNN in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 6\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu'))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/6\n",
      "60000/60000 [==============================] - 51s 848us/step - loss: 0.2688 - accuracy: 0.9183 - val_loss: 0.0638 - val_accuracy: 0.9794\n",
      "Epoch 2/6\n",
      "60000/60000 [==============================] - 55s 915us/step - loss: 0.0914 - accuracy: 0.9730 - val_loss: 0.0427 - val_accuracy: 0.9863\n",
      "Epoch 3/6\n",
      "60000/60000 [==============================] - 60s 1ms/step - loss: 0.0691 - accuracy: 0.9801 - val_loss: 0.0328 - val_accuracy: 0.9890\n",
      "Epoch 4/6\n",
      "60000/60000 [==============================] - 57s 942us/step - loss: 0.0569 - accuracy: 0.9829 - val_loss: 0.0329 - val_accuracy: 0.9893\n",
      "Epoch 5/6\n",
      "60000/60000 [==============================] - 54s 896us/step - loss: 0.0480 - accuracy: 0.9858 - val_loss: 0.0310 - val_accuracy: 0.9899\n",
      "Epoch 6/6\n",
      "60000/60000 [==============================] - 54s 904us/step - loss: 0.0423 - accuracy: 0.9872 - val_loss: 0.0317 - val_accuracy: 0.9899\n",
      "Test loss: 0.03167212410594293\n",
      "Test accuracy: 0.9898999929428101\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset is a great benchmark because it allows us to directly measure the performance of a neural\n",
    "network against the human eye. But often, the association between datapoint and label is not as clear, yet we\n",
    "still want to do prediction.\n",
    "\n",
    "I went on [Kaggle.com](www.kaggle.com) and looked for some *harder* datasets. At the time of writing there's\n",
    "[a competetion](https://www.kaggle.com/c/petfinder-adoption-prediction) where you can win up to $\\$$25.000 by\n",
    "predicting how quickly pets get adopted (on a pet-adoption site) based on meta data and images of the pets. From that competetion, I prepared a dataset of $128 \\times 128$ images along with adoption time (lower is better, see description on Kaggle). In the actual competetion people are using more data sources than just image, but for\n",
    "now, let's see how well we can do with just that. Here are links to download the prepared datasets ([train](https://www.dropbox.com/s/zjno7wpmlurw8bo/adoptability_data.zip?dl=0) // [test](https://www.dropbox.com/s/9m93dzhjc9f9jrr/adoptability_test.zip?dl=0))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.1.6**: Load the prepared dataset and build a CNN that predicts adoption-time category from the image.\n",
    "Leave out the last 1000 datapoints from training and save them for validation. You have complete freedom in how\n",
    "you want to construct the neural network (choice of cost function, inner architecture, training hyperparameters,\n",
    "etc.). Report an accuracy on the test set (last 1000 points) that's higher than the baseline (0.273). Using your\n",
    "adoptability predictor, **visualize the 5 most adoptable dogs**.\n",
    ">\n",
    "> As an extra bonus, I also included a test dataset with no corresponding target array. You can take your model and try to predict the test data, and format it similar to the `sample_submission.csv` file and submit your predictions to the competetion to get an estimate of the general performance of your model. **Let me know what you score if you do that**. And if you choose to join the competetion and try to win the money, I'd also like to hear about your progress. You can keep the money.\n",
    ">\n",
    "> Also, note that to get high accuracy, you need to make a network that is bigger than what you had in the previous exercise. This will take a long time to train until convergence (hours maybe). If you have a computer with a small\n",
    "CPU you will have to make do with a smaller neural network, and thus a lower accuracy. But you should still be able to score above the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you download the data and put in the same folder as this notebook,\n",
    "then the below code will prepare your data nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T20:43:42.180514Z",
     "start_time": "2019-09-30T20:43:31.937880Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "x = np.load('/Users/ulfaslak/Downloads/adoptability_data/x.npy').astype('float32')\n",
    "y = np.load('/Users/ulfaslak/Downloads/adoptability_data/y.npy').astype(int)\n",
    "\n",
    "# Transform images from 0-255 to 0-1 values\n",
    "x /= 255\n",
    "\n",
    "# Reshape data\n",
    "x = x.reshape(-1, 128, 128, 3)\n",
    "num_classes = len(set(y))\n",
    "y = keras.utils.to_categorical(y, num_classes)\n",
    "\n",
    "# Split into train and test\n",
    "x_test = x[-1000:]\n",
    "y_test = y[-1000:]\n",
    "x_train = x[:-1000]\n",
    "y_train = y[:-1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T20:44:02.132579Z",
     "start_time": "2019-09-30T20:44:02.127695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27300027\n"
     ]
    }
   ],
   "source": [
    "# Baseline\n",
    "print(max(y.sum(0))/y.sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
